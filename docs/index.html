<!doctype html>
<html lang="en">

<head>
    <!-- Required meta tags -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

    <!-- Bootstrap CSS -->
    <link rel="stylesheet" href="css/bootstrap.min.css">
    <!-- Custom styles -->
    <link href="style.css" rel="stylesheet">
    <link rel="icon" href="favicon.ico">
    <base target="_blank">

    <title>CS-UY 4563 Spring 2020</title>
</head>

<body>

    <nav class="navbar navbar-expand-md navbar-dark fixed-top bg-dark">
        <a class="navbar-brand" href="#">CS-UY 4563</a>
        <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarsExampleDefault"
            aria-controls="navbarsExampleDefault" aria-expanded="false" aria-label="Toggle navigation">
            <span class="navbar-toggler-icon"></span>
        </button>

        <div class="collapse navbar-collapse" id="navbarsExampleDefault">
            <ul class="navbar-nav mr-auto">
                <li class="nav-item">
                    <a class="nav-link" href="https://newclasses.nyu.edu/portal/site/b47347eb-6e37-4462-bc03-cea8811a8fe9">NYU Classes</a>
                </li>
                <!-- <li class="nav-item dropdown">
                    <a class="nav-link dropdown-toggle" id="dropdown01" data-toggle="dropdown" aria-haspopup="true"
                        aria-expanded="false">Previous
                        Years
                    </a>
                    <div class="dropdown-menu" aria-labelledby="dropdown01">
                        <a class="dropdown-item" href="https://www.cs.princeton.edu/~smattw/Teaching/cos521fa17.htm">2017</a>
                        <a class="dropdown-item" href="https://www.cs.princeton.edu/courses/archive/fall16/cos521/">2016</a>
                    </div>
                </li> -->
            </ul>
        </div>
    </nav>

    <main role="main">

        <!-- Main jumbotron for a primary marketing message or call to action -->
        <section class="jumbotron text-center">
            <div class="container">
                <h1 class="jumbotron-heading">NYU CS/ECE-UY 4563<br>Introduction to Machine Learning</h1>

                <p class="lead text-muted">
                    Advanced topics course exploring contemporary algorithmic techniques and recent research on computational methods that enable machine learning and data science at scale. 
                </p>

                <h5 class="instructors">
                    <b>Course Instructor:</b> <a href="https://www.chrismusco.com/">Professor Christopher Musco</a>
                </h5>
            </div>
        </section>

        <div class="album py-3 bg-light">
            <div class="container">
                <div class="row">
                </div>
                <div class="row">
                    <div class="col-sm">
                        <h3 class="maintopic"><br>Course Summary</h3>
                        <p class="courseinfo">
                            <b>Prerequisites:</b> This course is mathematically rigorous, and is intended for graduate students or advanced undergraduates. I require a previous course in machine learning (for example, CS-UY 4563, CS-GY 6923, or ECE-GY 6143) and a previous course in algorithm design and analysis (for example, CS-UY 2413, CS-GY 6033, or CS-GY 6043). Experience with linear algebra and probability is also necessary. Email the course instructor if you have questions about your preparation for the course! Undergraduates wishing to enroll will be registering for CS-UY 3943.
                        </p>
                        <p class="courseinfo">
                            <b>Coursework:</b> One meeting per week. 4 problem sets involving analysis and application of algorithmic methods learned in class, potentially with programming exercises to further explore lecture content (40% of grade). Midterm exam (20% of grade). Final exam (30% of grade). Class participation is the remaining 10% of the grade. Please consult the <a href="syllabus/syllabus.pdf"> formal syllabus</a> for more information.
                        </p>

                        <p class="courseinfo">
                            <b>Resources:</b> There is no textbook to purchase. Course material will consist of my written lecture notes, as well as assorted online resources, including papers, notes from other courses, and publicly available surveys. Please refer to the course webpage before and after lectures to keep up-to-date as new resources are posted.
                        </p>

                        <p>
                            <b>Homework:</b><br/>
                            <a href="homeworks/hw1.pdf">Homework 1</a> (due Thursday, Sept. 26th by 11:59pm).<br/>
                            <a href="homeworks/hw2.pdf">Homework 2</a> (due Thursday, Oct. 17th by 11:59pm).<br/>
                            <a href="homeworks/hw3.pdf">Homework 3</a>, <a href="homeworks/UScities.txt">UScities.txt</a> (due Monday, Dec. 2nd by 11:59pm).<br/>
                            <a href="homeworks/hw4.pdf">Homework 4</a> (due Monday, Dec. 16th by 11:59pm).
                        </p>
                    </div>
                    <div class="col-sm">
                        <h3 class="maintopic"><br>Administrative Information</h3>
                        <p class="courseinfo">
                            <b>Lectures:</b> Wednesdays 3:20-5:50pm in Rogers Hall,  RGSH 707.
                        </p>
                        <p class="courseinfo">
                            <b>Syllabus:</b> <a href="syllabus/syllabus.pdf"> here.</a>
                        </p>
                        <p class="courseinfo">
                            <b>Office hours:</b> Thursdays, 3-5pm, or by appointment. 370 Jay St., Office 1105 (11th floor). <br>
                            <b>Reading Group:</b> 10am Tuesdays, 370 Jay St. Room 1114.
                        </p>

                        <p class="courseinfo">
                            <b>Homework:</b> While not required, I encourage students to prepare problem sets in <a href="https://astrobites.org/2018/01/20/getting-started-with-latex/">LaTeX</a> or <a href="https://en.wikipedia.org/wiki/Markdown"> Markdown</a> (with <a href="http://support.typora.io/Math/">math support</a>.)
                            You can use this <a href="template.tex">template</a> for LaTeX. While there is a learning curve for LaTeX (less for Markdown), it typically saves students lots of time by the end of the semester!
                        </p>
                        <p class="courseinfo"></p>
                            For regular homework problems <u>collaboration is allowed, but solutions and any code must be written independently</u>. Students must list collaborators on their problem sets (for each problem separately). See the <a href="syllabus/syllabus.pdf">syllabus</a> for full details.
                        </p>

                        <p class="courseinfo"> 
                            <b>Optional Reading Group:</b> It's an exciting time for research at the intersection of algorithm design and the data sciences. Many of the ideas covered in this course are still the subject of active research. We currently hold a reading to discuss recent papers on <b>Tuesdays at 10am</b> in <b>Room 1114, 370 Jay St.</b>. If you are interested in participating in the reading group, please email me.
                        </p>
                    </div>
                </div>
            </div>
        </div>

        <div class="album py-3 bg-light">
            <div class="container">
                <table class="table">
                    <thead class="thead-dark">
                        <tr>
                            <th scope="col">Lecture #</th>
                            <th scope="col">Topic</th>
                            <th scope="col">Required Reading</th>
                            <th scope="col">Optional Reading</th>
                        </tr>
                    </thead>
                    <tbody>

                        <tr class="tablesection">
                            <td colspan="4">The Power of Randomness</td>
                        </tr>

                        <tr class="oddrow">
                            <th scope="row">1. 9/4</th>
                            <td>
                                Concentration of random variables + applications to hashing and load balancing
                            </td>
                            <td>
                                <a href="lectures/lec1.pdf">Lecture 1 notes.</a>
                            </td>
                            <td>
                                <ul>
                                    <li> In practice, efficient "pseudorandom" hash functions need to be used in place of "fully random" functions. See <a href = "https://www.cs.princeton.edu/courses/archive/fall18/cos521/Lectures/lec1.pdf">these notes</a> for  details on one class of simple hash functions called "2-universal" hash functions, which can be used for all applications discussed in class.
                                    </li>
                                </ul>
                            </td>
                        </tr>
                        <tr class="evenrow">
                            <th scope="row">2. 9/11</th>
                            <td>
                                Chernoff bounds + sketching and streaming algorithms
                            </td>
                            <td>
                                <a href="lectures/lec2.pdf">Lecture 2 notes.</a><br/>
                                <a href="lectures/lec2_annotated.pdf">(annotated)</a>
                            </td>
                            <td>
                                <ul>
                                    <li>For a proof of the "power of two choices'' result, see Section 2 in this <a
                                            href="http://www.eecs.harvard.edu/~michaelm/postscripts/handbook2001.pdf">survey</a>.
                                    </li>
                                    <li>Additional reading on concentration bounds can be found in Terry Tao's <a href="https://terrytao.wordpress.com/2010/01/03/254a-notes-1-concentration-of-measure/">notes</a>.
                                    </li>
                                    <li>
                                        See this <a href="http://static.googleusercontent.com/media/research.google.com/en/us/pubs/archive/40671.pdf">paper</a> for the latest on streaming distinct elements estimation in practice.
                                    </li>
                                </ul>
                            </td>
                        </tr>
                        <tr class="oddrow">
                            <th scope="row">3. 9/18</th>
                            <td>
                                Sketching, the Johnson-Lindenstrauss lemma + applications
                            </td>
                            <td>
                                <a href="lectures/lec3.pdf">Lecture 3 notes.</a><br/>
                                <a href="lectures/lec3_annotated.pdf">(annotated)</a>
                            </td>
                            <td>
                                <ul>
                                    <li><a href="http://infolab.stanford.edu/~ullman/mmds/ch3.pdf">Book chapter</a> on Jaccard similarity, MinHash, etc.</li>
                                    <li>Helpful <a href="https://www.cs.cmu.edu/~avrim/Randalgs11/lectures/lect0314.pdf">notes</a> on the JL Lemma by Anupam Gupta.</li>
                                    <li>Cool <a href="https://www.vldb.org/pvldb/vol11/p1674-rong.pdf">paper</a> on an application of MinHash to detecting seismic events.
                                    </li>
                                    <li>Original <a href="lectures/FlajoletDurand.pdf">paper</a> giving a loglog(n) space algorithm for the distinct elements problem.
                                    </li>
                                </ul>
                            </td>
                        </tr>
                        <tr class="evenrow">
                            <th scope="row">4. 9/25</th>
                            <td>
                                Nearest neighbor search + locality sensitive hashing
                            </td>
                            <td>
                                <a href="lectures/lec4.pdf">Lecture 4 notes.</a><br/>
                                <a href="lectures/lec4_annotated.pdf">(annotated)</a>
                            </td>
                            <td>
                                <ul>
                                    <li>Good overview of similarity estimation and locality sensitive hashing in Chapter 3 <a href="http://www.mmds.org/">here</a>. There are also <a href="http://web.stanford.edu/class/cs246/slides/03-lsh.pdf">lecture notes</a>. These resources use slightly different language (and a slightly different version of MinHash) than I used in class.
                                    </li>
                                </ul>
                            </td>    
                        </tr>
                        <tr class="tablesection">
                                <td colspan="4">First-Order Optimization</td>
                        </tr>

                        <tr class="oddrow">
                            <th scope="row">5. 10/2</th>
                            <td>
                                Convexity in machine learning + vanilla, stochastic, and online gradient descent
                            </td>
                            <td>
                                <a href="lectures/lec5.pdf">Lecture 5 notes.</a><br/>
                                <a href="lectures/lec5_annotated.pdf">(annotated)</a>
                            </td>
                            <td>
                                <ul>
                                    <li>If you need freshen up on linear algebra, now is good time! This <a href="http://web.stanford.edu/class/cs246/handouts/CS246_LinAlg_review.pdf">quick reference</a> from Stanford mostly covers what we need.
                                    </li>
                                    <li>
                                        Good <a href="https://link.springer.com/book/10.1007/978-0-387-40065-5">book</a> on optimization which is freely available online through NYU libraries.
                                    </li>
                                    <li>
                                        Excellent <a href="https://people.csail.mit.edu/madry/6S978/">lecture notes</a> from Aleksander Mądry for more reading on analyzing gradient descent.
                                    </li>

                                    
                                </ul>
                            </td>
                        </tr>

                        <tr class="evenrow">
                            <th scope="row">6. 10/9</th>
                            <td>
                                Finishing up SGD, smoothness, strong convexity, conditioning.
                            </td>
                            <td>
                                <a href="lectures/lec6.pdf">Lecture 6 notes.</a><br/>
                                <a href="lectures/lec6_annotated.pdf">(annotated)</a>
                            </td>
                            <td>
                                <ul>
                                    <li>Moritz Hardt's <a href="https://ee227c.github.io/notes/ee227c-notes.pdf">lecture notes</a> with proofs of gradient descent convergence in all the regimes discussed in class.
                                    </li>
                                </ul>
                            </td>
                        </tr>

                        <tr class="oddrow">
                            <th scope="row">7. 10/16</th>
                            <td> 
                                Preconditioning, acceleration, randomized coordinate descent, adaptive methods.
                            </td>
                            <td>
                                <a href="lectures/lec7.pdf">Lecture 7 notes.</a>
                                <a href="lectures/lec7_annotated.pdf">(annotated)</a>
                            </td>
                            <td>
                                <ul>
                                    <li>See Section 5 (rest is not relevant) of Aleksander Mądry's lecture <a href="https://people.csail.mit.edu/madry/6S978/files/lecture_7.pdf">here</a> for an overview of preconditioning, which will be covered in class.
                                    </li>
                                    <li>Moritz Hardt's <a href="https://ee227c.github.io/notes/ee227c-notes.pdf">lecture notes</a> are a good reference for coordinate decent and acceleration, which will be covered if we have time.
                                    </li>
                                    <li><a href=" https://blogs.princeton.edu/imabandit/2014/03/06/nesterovs-accelerated-gradient-descent-for-smooth-and-strongly-convex-optimization/">Proof</a> of accelerated gradient descent for general convex functions.
                                    </li>
                                </ul>
                            </td>

                            
                        </tr>

                        <tr class="evenrow">
                                <th scope="row">8. 10/23</th>
                                <td> 
                                    <b>Midterm exam</b> (first half of class) </br></br>
                                    Finishing up coordinate descent, optimization beyond convexity (second half of class)
                                </td>
                                <td>
                                    <a href="lectures/lec8.pdf">Lecture 8 notes.</a>
                                    <a href="lectures/lec8_annotated.pdf">(annotated)</a>
                                </td>
                                <td>
                                    <ul>
                                        <li>
                                            Diverse, theory-leaning <a href="https://www.offconvex.org/">blog</a> which discusses lots of problems and approaches in understanding the optimization of non-convex problems.
                                        </li>
                                        <li>
                                            Another useful <a href="http://cs229.stanford.edu/section/cs229-linalg.pdf">document</a> for linear algebra review. Section 3 is especially important.
                                        </li>
                                    </ul>
                                </td>
                        </tr>

                        <tr class="tablesection">
                                <td colspan="4">Spectral Methods and Linear Algebra</td>
                        </tr>

                        <!-- Learning from experts + multiplicative weights -->
                        <tr class="oddrow">
                                <th scope="row">9. 10/30</th>
                                <td> 
                                    Singular value decomposition, Power Method, Krylov methods
                                </td>
                                <td>
                                    <a href="lectures/lec9.pdf">Lecture 9 notes.</a>
                                    <a href="lectures/lec9_annotated.pdf">(annotated)</a>
                                </td>
                                <td>
                                    <ul>
                                    <li>Read <a href="https://www.cs.cornell.edu/jeh/book.pdf}">Foundations of Data Science, Chapter 3</a> for more on singular value decomposition and low-rank approximation.</li>
                                    <li>Other good notes <a href="http://infolab.stanford.edu/~ullman/mmds/ch11.pdf">here</a> and <a href="http://web.stanford.edu/class/cs168/l/l9.pdf">here</a>.</li>
                                </ul>
                                </td>
                        </tr>

                        <tr class="evenrow">
                            <th scope="row">10. 11/6</th>
                            <td> 
                                Finish up Krylov methods, spectral clustering, and spectral graph theory.
                            </td>
                            <td>
                                <a href="lectures/lec10.pdf">Lecture 10 notes.</a>
                                <a href="lectures/lec10_annotated.pdf">(annotated)</a>
                            </td>
                            <td>
                                <ul>
                                    <li>
                                        See Section 3.7 in <a href="https://www.cs.cornell.edu/jeh/book.pdf}">Foundations of Data Science</a> for an analysis of power method that does not involve singular value gaps.
                                    </li>
                                    <li>
                                        Stanford <a href="http://web.stanford.edu/class/cs168/l/l11.pdf">lecture notes</a> introducing spectral graph theory.
                                    </li>
                                    <li>
                                        See Section 6.4 in <a href="https://arxiv.org/pdf/1309.4882.pdf">Approximation Theory and the Design of Fast Algorithms</a> for a simple analysis of the Lanczos method.
                                    </li>
                                </ul>
                            </td>
                            <td></td>
                        </tr>
                        <tr class="oddrow">
                            <th scope="row">11. 11/13</th>
                            <td>
                                Finish spectral graph theory, start randomized linear algebra, sketching for linear regression, &epsilon;-nets arguments
                            </td>
                            <td>
                                <a href="lectures/lec11.pdf">Lecture 11 notes.</a>
                                <a href="lectures/lec11_annotated.pdf">(annotated)</a>
                            </td>
                            <td>
                                <ul>
                                <li>
                                    For more details, consult previous years notes on the <a href="https://www.cs.princeton.edu/courses/archive/fall18/cos521/Lectures/lec14.pdf">stochastic block model</a> and <a href="https://www.cs.princeton.edu/courses/archive/fall18/cos521/Lectures/lec11.pdf">subspace embeddings for linear regression</a>.
                                </li>
                                </ul>
                            </td>
                        </tr>

                        <tr class="evenrow">
                            <th scope="row">12. 11/20</th>
                            <td>
                                Randomized numerical linear algebra linear, fast Johnson-Lindenstrauss transform, sampling.
                            </td>
                            <td>
                                <a href="lectures/lec12.pdf">Lecture 12 notes.</a>
                                <a href="lectures/lec12_annotated.pdf">(annotated)</a>
                            </td>
                            <td>
                                <ul>
                                    <li>
                                        Jelani Nelson's <a href="http://people.seas.harvard.edu/~minilek/cs229r/fall15/lec.html">course notes</a> from Harvard with a lot more on randomized linear algebra, including methods for sparse JL sketching and randomized low-rank approximation. 
                                    </li>
                                    <li>
                                        &epsilon;-net arguments are used all over the place in learning theory, algorithm design, and high dimensional probability. Here's an example of how they appear in a
                                        <a href="https://www2.cs.duke.edu/courses/spring07/cps296.2/scribe_notes/lecture09.pdf">different context</a>.
                                    </li>
                                </ul>
                            </td>
                        </tr>
                        <tr class="offday oddrow">
                            <th scope="row">11/27</th>
                            <td>No Class, Thanksgiving</td>
                            <td></td>
                            <td></td>
                        </tr>

                        <tr class="tablesection">
                                <td colspan="4">Fourier Methods</td>
                        </tr>
                        <tr class="evenrow">
                            <th scope="row">13.  12/4</th>
                            <td>
                                Compressed sensing, the restricted isometry property, basis pursuit, the discrete Fourier transform
                            </td>
                            <td>
                                <a href="lectures/lec13.pdf">Lecture 13 notes.</a>
                                <a href="lectures/lec13_annotated.pdf">(annotated)</a>
                            </td>
                            <td>
                                <ul>
                                    <li>
                                        Last year's <a href="https://www.cs.princeton.edu/courses/archive/fall18/cos521/Lectures/lec21.pdf">lecture notes</a> on compressed sensing and basis pursuit.
                                    </li>
                                    <li>
                                        Lots of great resources under weeks 8 and 9 of Stanford's
                                        <a href="https://web.stanford.edu/class/cs168/">Algorithmic Toolbox</a> course.
                                    </li>
                                    <li>
                                        <a href="https://groups.csail.mit.edu/netmit/sFFT/">Webpage</a> on Sparse Fourier Transform algorithms and some applications. 
                                    </li>
                                </ul>
                            </td>
                        </tr>
                        <tr class="oddrow">
                            <th scope="row">14. 12/11</th>
                            <td>High dimensional geometry, maybe some more Fourier methods</td>
                            <td>
                                <a href="lectures/lec14.pdf">Lecture 14 notes.</a>
                                <a href="lectures/lec14_annotated.pdf">(annotated)</a>
                            </td>
                            <td></td>
                        </tr>
                        <tr class="oddrow">
                            <th scope="row">15. 12/18</th>
                            <td><b>Final exam</b></td>
                            <td></td>
                            <td></td>
                        </tr>
                    </tbody>
                </table>

            </div>
        </div>

    </main>

    <!-- <footer class="container">
        <p>&copy 2017-2018</p>
    </footer> -->

    <!-- Optional JavaScript -->
    <!-- jQuery first, then Popper.js, then Bootstrap JS -->
    <script src="js/jquery-3.3.1.min.js"></script>
    <script src="js/bootstrap.min.js"></script>
</body>

</html>